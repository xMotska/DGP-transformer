{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44de66e-1ccd-4a41-a2a6-a3d79712cdb1",
   "metadata": {},
   "source": [
    "# Data generating module\n",
    "Generate the synthetic data and then save using pickle (useful for importing to the other environment due to potential library conflicts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefe986a-7568-4e18-a10b-b389f89215db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 19:35:22.180717: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-10 19:35:24.490657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-10 19:35:25.232352: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-10 19:35:25.236569: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-10 19:35:26.515334: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-10 19:35:34.746192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/xristos/miniconda3/envs/datageneration/lib/python3.10/site-packages/gpflow/versions.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import gpflux\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gpflux.helpers import construct_basic_kernel, construct_basic_inducing_variables\n",
    "from gpflux.layers import GPLayer\n",
    "from gpflux.experiment_support.plotting import plot_layer\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import ruptures as rpt\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b5c6f-bbc8-4cdc-afea-68302d81cbcd",
   "metadata": {},
   "source": [
    "## Poisson Point Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4784e99-dabb-4c95-a9a8-4f9cdf110d8f",
   "metadata": {},
   "source": [
    "We start by defining an intensity function for the Non-Homogeneous Poisson process (NHPP) and then generate a Point process using a thinning algorithm that can be found here (https://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-PP-NSPP.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1226f2a-201e-40e5-b050-393c8805a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lam(x, a):\n",
    "    \"\"\"\n",
    "    Intensity function for the NHPP\n",
    "    \"\"\"\n",
    "    return a + np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042f7b1c-0058-4fa9-b8f7-08dc1c90bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_poisson_jumps(a=10, lambda_star=11, length=1000, seed=None, segment_length=256):\n",
    "    \"\"\"\n",
    "    Thinning algorithm for generating a poisson point process signal\n",
    "    that alternates monotonicity every segment_length points.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    T = 0.01 * length\n",
    "    t = 0.0\n",
    "    n = 0\n",
    "    arrival_times = []\n",
    "    while True:\n",
    "        u = np.random.rand()\n",
    "        t = t - (1/lambda_star) * np.log(u)\n",
    "        if t > T:\n",
    "            break\n",
    "        u = np.random.rand()\n",
    "        lambda_t = lam(t, a)\n",
    "        if u <= lambda_t / lambda_star:\n",
    "            n += 2\n",
    "            arrival_times.append(t)\n",
    "    ts = np.linspace(0, T, length)\n",
    "    ys = np.searchsorted(arrival_times, ts)\n",
    "    \n",
    "\n",
    "    increments = np.diff(ys, prepend=ys[0])\n",
    "    \n",
    "    segment_indices = np.arange(length) // segment_length\n",
    "    signs = np.where(segment_indices % 2 == 0, 1, -1)\n",
    "    ys_alternating = np.cumsum(increments * signs)\n",
    "    \n",
    "    return ts, ys_alternating, arrival_times, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde5e32a-d0fd-4768-b946-249923503a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multiple_poisson(a, lambda_star, length, num_samples, seed=None):\n",
    "    \"Sample many Poisson base signals\"\n",
    "    signals = []\n",
    "    tau = None\n",
    "    tss = None\n",
    "    for i in range(num_samples):\n",
    "        current_seed = seed + i if seed is not None else None\n",
    "        ts, ys, arrival_times, T = sample_poisson_jumps(a, lambda_star, length, current_seed)\n",
    "        signals.append(ys)\n",
    "        tau = T\n",
    "        tss = ts\n",
    "    return np.stack(signals, axis=0), tau, tss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b50ee-db66-4985-9c7c-7d8db19c56be",
   "metadata": {},
   "source": [
    "## Deep Gaussian Process (DGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71edf583-fce1-4420-8114-a1d1099a7d03",
   "metadata": {},
   "source": [
    "We continue with defining the hierarchical structure of the signals. We start by defining a helper function for sampling layer kernels and hyperparameters. The DGP model is constructed by first sampling from two stationary kernels supported by GPFlow for each layer while fixing the lengthscales and variance hyperparameters to 0.5 and 1.0 respectively. We then create a set of datapoints that are propagated through a number of predefined layers and save the output of the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b23ece-b800-41a2-bef3-512eac20005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kernels we will be using for the layers\n",
    "KERNELS = [gpflow.kernels.SquaredExponential,\n",
    "          gpflow.kernels.Matern12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9b96ca-4334-4e5c-a643-d48ff54c1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_sample_kernels(kernels, num_kernels, seed=None):\n",
    "    \"\"\"Helper function for sampling a corresponding kernel for each DGP layer.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    sampled_kernels = []\n",
    "    for i in range(num_kernels):\n",
    "        kernel = np.random.choice(kernels)()\n",
    "        lengthscales = np.random.uniform(0.5, 0.5)\n",
    "        variance = np.random.uniform(1.0, 1.0)\n",
    "        kernel.lengthscales.assign(lengthscales)\n",
    "        kernel.variance.assign(variance)\n",
    "        sampled_kernels.append(kernel)\n",
    "    return sampled_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3670ab-4d94-46aa-9807-8c5ca9c1f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dgp_signal(segment_length, a, b, num_samples, kernels, num_layers, seed=None, reverse=False):\n",
    "    \"\"\"\n",
    "    Sample num_samples trajectories where each segment of segment_length \n",
    "    corresponds to a DGP layer output.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "    \n",
    "    X = np.linspace(a, b, segment_length).reshape(-1, 1)\n",
    "    Z = X.copy()\n",
    "    M = Z.shape[0]\n",
    "    D = 1\n",
    "    \n",
    "    gp_layers = []\n",
    "    for i in range(num_layers):\n",
    "        ind_var = construct_basic_inducing_variables(M, D, D, share_variables=True, z_init=Z.copy())\n",
    "        kernel = construct_basic_kernel(kernels[i], output_dim=D, share_hyperparams=True)\n",
    "        layer_num_samples = num_samples if i == 0 else 1\n",
    "        gplayer = GPLayer(\n",
    "            kernel,\n",
    "            ind_var,\n",
    "            segment_length,\n",
    "            full_cov=True,\n",
    "            num_samples=layer_num_samples,\n",
    "            mean_function=gpflow.mean_functions.Zero(),\n",
    "        )\n",
    "        gp_layers.append(gplayer)\n",
    "    \n",
    "    layer_input = X\n",
    "    all_segments = []\n",
    "    \n",
    "    for i, layer in enumerate(gp_layers):\n",
    "        layer_output = layer(layer_input)\n",
    "        sample = tf.convert_to_tensor(layer_output)\n",
    "        \n",
    "        if i == 0:\n",
    "            layer_input = sample\n",
    "        else:\n",
    "            layer_input = sample[0]\n",
    "        \n",
    "        segment = np.squeeze(layer_input.numpy(), axis=-1)\n",
    "        all_segments.append(segment)\n",
    "    \n",
    "    # Reverse if requested: [L1, L2, L3, L4] → [L4, L3, L2, L1]\n",
    "    if reverse:\n",
    "        all_segments = all_segments[::-1]\n",
    "    \n",
    "    final = np.concatenate(all_segments, axis=1)\n",
    "    \n",
    "    total_length = segment_length * num_layers\n",
    "    xs = np.linspace(a, b * num_layers, total_length)\n",
    "    \n",
    "    return final, all_segments, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbe8653-f91b-4b85-8d32-d05ca81fa80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences(a, lambda_star, length, kernels, num_layers, num_samples,poisson_seed=None, kernel_seed=None, dgp_seed=None):\n",
    "    segment_length = length // num_layers\n",
    "    baselines, T, xs = sample_multiple_poisson(a, lambda_star, length, num_samples, poisson_seed)\n",
    "    final, all_segments, xs = sample_dgp_signal(segment_length, 0, T, num_samples, kernels, num_layers, dgp_seed, reverse=True)\n",
    "    seqs = baselines + final\n",
    "    return T, baselines, seqs, all_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a7ff8-27f5-478a-b9a0-1dfd7fe6c166",
   "metadata": {},
   "source": [
    "## We can now generate the time series we will need to train and evaluate our models on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fdb79-1399-4442-87e8-545954fdf6fa",
   "metadata": {},
   "source": [
    "We sample one synthetic dataset of num_samples = 1024 and seq_len = 1024. We produce the flat structure as described in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f482101-8576-4448-8272-ee3574037f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f876b235-38cf-414c-bf48-5d67d57f8ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 22:48:52.674396: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "seed = 500\n",
    "kernels = randomly_sample_kernels(KERNELS, 4, seed)\n",
    "T, baselines, samples, segments = sequences(4, 5, 1024, kernels, 4, 1024, seed, seed, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72cb34e-5939-4a78-9beb-72d876ae925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"dgp_dataset\"] = {\"T\": T, \"baselines\": baselines, \"samples\":samples, \"segments\":segments}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a152c70-4024-4fe5-97b7-0ca099174381",
   "metadata": {},
   "source": [
    "## We can now save the dataset in order to import it to another environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa4a6966-ae7e-4fac-b3d4-cdb2b0aeda88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/synthetic_datasets_segments.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_path = \"data/synthetic_datasets_segments.pkl\"\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9cee1-cd49-45cb-b28d-f9e93f5879e2",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "\n",
    "After training and prediction, we are able to extract meaningful statistics out of the generated sequences which we can then use to quantitatively compare the outputs with the ground truth. First, we start with mapping each signal to a triple contatining changepoint count, jump mean differences and segment variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3950084-2a2a-42b7-aa7a-b53815c668ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cp(signal, pen=10):\n",
    "    \"\"\"Helper function which returns a ruptures model using the Pelt algorithm, an L2 segment cost function with a penalty (default 10).\"\"\"\n",
    "    return rpt.Pelt(model='l2').fit(signal).predict(pen=pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d38485b2-7888-40dd-8108-3101929f2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal_statistics(ys):\n",
    "    # First, estimate the changepoints\n",
    "    cps = estimate_cp(ys)\n",
    "    cps = np.asarray(cps, dtype=int)\n",
    "    \n",
    "    # Get the boundaries of each changepoint\n",
    "    boundaries = np.concatenate(([0], cps))\n",
    "    inter_arrivals = np.diff(boundaries)\n",
    "    num_cps = len(cps)\n",
    "\n",
    "    means = []\n",
    "    variances = []\n",
    "    lengths = inter_arrivals\n",
    "    segments = [(boundaries[i],boundaries[i+1]) for i in range(len(boundaries)-1)]\n",
    "    for (start, end) in segments:\n",
    "        segment = ys[start:end]\n",
    "        means.append(segment.mean())\n",
    "        variances.append(segment.var())\n",
    "    # Local statistics, means, variances and lengths\n",
    "    means = np.asarray(means)\n",
    "    variances = np.asarray(variances)\n",
    "\n",
    "    # Jumps in means\n",
    "    jumps = np.diff(means)\n",
    "\n",
    "\n",
    "    statistics =  {\n",
    "        \"cps\" : cps,\n",
    "        \"num_cps\" : num_cps,\n",
    "        \"inter_arrivals\": inter_arrivals,\n",
    "        \"lengths\": lengths,\n",
    "        \"means\": means,\n",
    "        \"variances\": variances,\n",
    "        \"jumps\": jumps,\n",
    "        }\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda830a-aabb-4f3a-9683-193aecd20938",
   "metadata": {},
   "source": [
    "## In-segment evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d30d6948-97b5-48eb-af5b-5a85cf9f8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/predictions/in_segment.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "with open(\"data/synthetic_datasets_segments.pkl\", \"rb\") as f:\n",
    "    samples = pickle.load(f)\n",
    "signals = samples[\"dgp_dataset\"][\"samples\"][819:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb5aa01f-4d57-4789-989d-a5183d190a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [3040 + i for i in range(10)]\n",
    "segments = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5adcb-630a-45b6-ab25-3f2eb4b5eb75",
   "metadata": {},
   "source": [
    "First, we evaluate the statistics of the ground truth signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "682fe7e6-69a8-4396-aa72-e9c7cfbef0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_stats = {}\n",
    "for seg_idx in segments:\n",
    "    seg_start = (seg_idx - 1) * 256\n",
    "    gt_stats[seg_idx] = []\n",
    "    for j in range(signals.shape[0]): # go through all 205 signals.\n",
    "        gt_signal = signals[j, seg_start + 128 : seg_start + 256]\n",
    "        stats = extract_signal_statistics(gt_signal)\n",
    "        gt_stats[seg_idx].append(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505f64e-7dc0-4b5b-87eb-0b0c3a668db3",
   "metadata": {},
   "source": [
    "Then, those of the predicted signals. These correspond to the 2nd half of the number of time points in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1850f056-6930-40ec-a87c-058acb93f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stats = {}\n",
    "for seed in seeds:\n",
    "    pred_stats[seed] = {}\n",
    "    for seg_idx in segments:\n",
    "        pred_stats[seed][seg_idx] = []\n",
    "        for j in range(signals.shape[0]):\n",
    "            # Get the 2nd half of the signal, that is the predicted section\n",
    "            pred_signal = results[seed][f\"segment{seg_idx}\"][j, 128:]\n",
    "            stats = extract_signal_statistics(pred_signal)\n",
    "            pred_stats[seed][seg_idx].append(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aac8eb-55c8-4868-9c35-7557a7443305",
   "metadata": {},
   "source": [
    "Now we can define the comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c59cfabc-6e75-414b-abd0-52ba9e8d6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = {}\n",
    "for seed in seeds:\n",
    "    evaluation[seed] = {}\n",
    "    for seg_idx in segments:\n",
    "        # look through all segments with the same segment index\n",
    "        gt_K = np.array([s[\"num_cps\"] for s in gt_stats[seg_idx]])\n",
    "        pred_K = np.array([s[\"num_cps\"] for s in pred_stats[seed][seg_idx]])\n",
    "        mae_K = np.mean(np.abs(gt_K - pred_K)) # 1) MAE\n",
    "        gt_jumps = np.concatenate([s[\"jumps\"] for s in gt_stats[seg_idx]])\n",
    "        pred_jumps = np.concatenate([s[\"jumps\"] for s in pred_stats[seed][seg_idx]])\n",
    "        gt_vars = np.concatenate([s[\"variances\"] for s in gt_stats[seg_idx]])\n",
    "        pred_vars = np.concatenate([s[\"variances\"] for s in pred_stats[seed][seg_idx]])\n",
    "        w1_jumps = wasserstein_distance(gt_jumps, pred_jumps) # 2) W1(delta)\n",
    "        w1_vars = wasserstein_distance(gt_vars, pred_vars) # 3) W1(sigma^2)\n",
    "        evaluation[seed][seg_idx] = {\n",
    "            \"mae_K\": mae_K,\n",
    "            \"w1_jumps\": w1_jumps,\n",
    "            \"w1_vars\": w1_vars,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eafb1b-eedc-4d31-95e9-cb31796eaa9e",
   "metadata": {},
   "source": [
    "We use the metrics as defined in the thesis to get qualitative comparisons between ground truth and generated signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95728b9d-39d7-4216-9524-e4ad0dcd52bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D=1 MAE:1.5639 (+-) 0.1531 | W1(delta): 3.1508 (+-) 0.1380 | W1(sigma): 0.5371 (+-) 0.2831\n",
      "D=2 MAE:1.6468 (+-) 0.2515 | W1(delta): 2.6030 (+-) 0.2039 | W1(sigma): 0.5860 (+-) 0.6593\n",
      "D=3 MAE:1.8132 (+-) 0.1021 | W1(delta): 2.8115 (+-) 0.1461 | W1(sigma): 1.2780 (+-) 1.0447\n",
      "D=4 MAE:1.8259 (+-) 0.1337 | W1(delta): 4.8902 (+-) 2.2672 | W1(sigma): 58.5480 (+-) 65.0997\n"
     ]
    }
   ],
   "source": [
    "for seg_idx in segments:\n",
    "    mae_vals = [evaluation[seed][seg_idx][\"mae_K\"] for seed in seeds]\n",
    "    w1j_vals = [evaluation[seed][seg_idx][\"w1_jumps\"] for seed in seeds]\n",
    "    w1v_vals = [evaluation[seed][seg_idx][\"w1_vars\"] for seed in seeds]\n",
    "    print(f\"D={seg_idx} MAE:{np.mean(mae_vals):.4f} (+-) {np.std(mae_vals):.4f} | W1(delta): {np.mean(w1j_vals):.4f} (+-) {np.std(w1j_vals):.4f} | W1(sigma): {np.mean(w1v_vals):.4f} (+-) {np.std(w1v_vals):.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e4c0d-6cdb-4646-9b09-1ae704d57cd3",
   "metadata": {},
   "source": [
    "Investigate a possible monotonic reversal trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a02256f8-d6d6-48ea-b97a-69004524e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D=1  W1(Δ) raw=3.1508 (+-) 0.1380 | W1(|Δ|) abs=0.2062 (+-) 0.0866\n",
      "D=2  W1(Δ) raw=2.6030 (+-) 0.2039 | W1(|Δ|) abs=0.3972 (+-) 0.1784\n",
      "D=3  W1(Δ) raw=2.8115 (+-) 0.1461 | W1(|Δ|) abs=0.3640 (+-) 0.1134\n",
      "D=4  W1(Δ) raw=4.8902 (+-) 2.2672 | W1(|Δ|) abs=2.7341 (+-) 2.3553\n"
     ]
    }
   ],
   "source": [
    "for seg_idx in segments:\n",
    "    gt_jumps = np.concatenate([s[\"jumps\"] for s in gt_stats[seg_idx]])\n",
    "    raw_w1 = []\n",
    "    abs_w1 = []\n",
    "    for seed in seeds:\n",
    "        pred_jumps = np.concatenate([s[\"jumps\"] for s in pred_stats[seed][seg_idx]])\n",
    "        raw_w1.append(wasserstein_distance(gt_jumps, pred_jumps))\n",
    "        abs_w1.append(wasserstein_distance(np.abs(gt_jumps), np.abs(pred_jumps)))\n",
    "    print(f\"D={seg_idx}  W1(Δ) raw={np.mean(raw_w1):.4f} (+-) {np.std(raw_w1):.4f} | W1(|Δ|) abs={np.mean(abs_w1):.4f} (+-) {np.std(abs_w1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af5d31-7d1e-44ce-bf2c-94c07cda3c3c",
   "metadata": {},
   "source": [
    "Further evidence for a monotonic reversal, get the signs of the jumps in mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "361181f6-7633-4483-9811-894e8f32fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D=1: \n",
      "  Ground truth:   mean=1.7913, std=0.6823\n",
      "  Pred: mean=-1.3410 ± 0.1446\n",
      " D=2: \n",
      "  Ground truth:   mean=-1.4600, std=0.9659\n",
      "  Pred: mean=1.0785 ± 0.1847\n",
      " D=3: \n",
      "  Ground truth:   mean=1.6733, std=1.1656\n",
      "  Pred: mean=-1.0322 ± 0.1427\n",
      " D=4: \n",
      "  Ground truth:   mean=-1.4248, std=1.3257\n",
      "  Pred: mean=1.0009 ± 0.3842\n"
     ]
    }
   ],
   "source": [
    "for seg_idx in segments:\n",
    "    gt_jumps = np.concatenate([s[\"jumps\"] for s in gt_stats[seg_idx]])\n",
    "    \n",
    "    all_pred_means = []\n",
    "    all_pred_stds = []\n",
    "    for seed in seeds:\n",
    "        pred_jumps = np.concatenate([s[\"jumps\"] for s in pred_stats[seed][seg_idx]])\n",
    "        all_pred_means.append(np.mean(pred_jumps))\n",
    "        all_pred_stds.append(np.std(pred_jumps))\n",
    "    print(f\" D={seg_idx}: \")\n",
    "    print(f\"  Ground truth:   mean={np.mean(gt_jumps):.4f}, std={np.std(gt_jumps):.4f}\")\n",
    "    print(f\"  Pred: mean={np.mean(all_pred_means):.4f} ± {np.std(all_pred_means):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04bf2a8c-7ee5-45dc-9668-2015ea3f33b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D=4: W1(|Δ|)=0.2062 ± 0.0866 | W1(|Δ|) clipped=0.1910 ± 0.0819 | Outliers=1.3%\n",
      "D=3: W1(|Δ|)=0.3972 ± 0.1784 | W1(|Δ|) clipped=0.3333 ± 0.1243 | Outliers=4.6%\n",
      "D=2: W1(|Δ|)=0.3640 ± 0.1134 | W1(|Δ|) clipped=0.2364 ± 0.0532 | Outliers=3.7%\n",
      "D=1: W1(|Δ|)=2.7341 ± 2.3553 | W1(|Δ|) clipped=0.3755 ± 0.1809 | Outliers=15.5%\n"
     ]
    }
   ],
   "source": [
    "for seg_idx in segments:\n",
    "    gt_jumps = np.concatenate([s[\"jumps\"] for s in gt_stats[seg_idx] if len(s[\"jumps\"]) > 0])\n",
    "    gt_abs_max = np.abs(gt_jumps).max()\n",
    "    \n",
    "    raw_w1 = []\n",
    "    abs_w1 = []\n",
    "    clipped_abs_w1 = []\n",
    "    outlier_pcts = []\n",
    "    for seed in seeds:\n",
    "        pred_jumps = np.concatenate([s[\"jumps\"] for s in pred_stats[seed][seg_idx] if len(s[\"jumps\"]) > 0])\n",
    "        \n",
    "        raw_w1.append(wasserstein_distance(gt_jumps, pred_jumps))\n",
    "        abs_w1.append(wasserstein_distance(np.abs(gt_jumps), np.abs(pred_jumps)))\n",
    "        \n",
    "        # Clip absolute jumps to GT range\n",
    "        pred_abs = np.abs(pred_jumps)\n",
    "        outlier_pcts.append(100 * np.sum(pred_abs > gt_abs_max) / len(pred_abs))\n",
    "        pred_clipped = np.clip(pred_abs, 0, gt_abs_max)\n",
    "        clipped_abs_w1.append(wasserstein_distance(np.abs(gt_jumps), pred_clipped))\n",
    "    \n",
    "    print(f\"D={5-seg_idx}: \"\n",
    "          f\"W1(|Δ|)={np.mean(abs_w1):.4f} ± {np.std(abs_w1):.4f} | \"\n",
    "          f\"W1(|Δ|) clipped={np.mean(clipped_abs_w1):.4f} ± {np.std(clipped_abs_w1):.4f} | \"\n",
    "          f\"Outliers={np.mean(outlier_pcts):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d02cf3b-987c-4539-8c83-0b418e7426e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D=4:\n",
      "  GT:   median=0.6351, mean=0.7302, std=0.3927, range=[0.0828, 2.7961]\n",
      "  Pred: median=0.5025 ± 0.1454, mean=1.0003 ± 0.4100, std=3.1500 ± 1.5932\n",
      "D=3:\n",
      "  GT:   median=0.5647, mean=0.6246, std=0.3409, range=[0.0126, 2.5544]\n",
      "  Pred: median=0.5169 ± 0.1008, mean=1.0970 ± 0.7076, std=8.1770 ± 11.9512\n",
      "D=2:\n",
      "  GT:   median=0.3768, mean=0.4165, std=0.2483, range=[0.0083, 3.0486]\n",
      "  Pred: median=0.2791 ± 0.0658, mean=1.5377 ± 1.0576, std=13.1492 ± 13.1270\n",
      "D=1:\n",
      "  GT:   median=0.2033, mean=0.2360, std=0.1682, range=[0.0010, 1.3212]\n",
      "  Pred: median=0.2797 ± 0.0672, mean=58.7680 ± 65.0933, std=427.8630 ± 395.0301\n"
     ]
    }
   ],
   "source": [
    "for seg_idx in segments:\n",
    "    gt_vars = np.concatenate([s[\"variances\"] for s in gt_stats[seg_idx] if len(s[\"variances\"]) > 0])\n",
    "    \n",
    "    all_pred_medians = []\n",
    "    all_pred_means = []\n",
    "    all_pred_stds = []\n",
    "    for seed in seeds:\n",
    "        pred_vars = np.concatenate([s[\"variances\"] for s in pred_stats[seed][seg_idx] if len(s[\"variances\"]) > 0])\n",
    "        all_pred_medians.append(np.median(pred_vars))\n",
    "        all_pred_means.append(np.mean(pred_vars))\n",
    "        all_pred_stds.append(np.std(pred_vars))\n",
    "    \n",
    "    print(f\"D={5-seg_idx}:\")\n",
    "    print(f\"  GT:   median={np.median(gt_vars):.4f}, mean={np.mean(gt_vars):.4f}, \"\n",
    "          f\"std={np.std(gt_vars):.4f}, range=[{gt_vars.min():.4f}, {gt_vars.max():.4f}]\")\n",
    "    print(f\"  Pred: median={np.mean(all_pred_medians):.4f} ± {np.std(all_pred_medians):.4f}, \"\n",
    "          f\"mean={np.mean(all_pred_means):.4f} ± {np.std(all_pred_means):.4f}, \"\n",
    "          f\"std={np.mean(all_pred_stds):.4f} ± {np.std(all_pred_stds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e019bd-5401-4207-a99f-d44de6632648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
