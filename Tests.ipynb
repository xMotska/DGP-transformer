{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c2625f-aaa5-4ecd-91cb-b79b5474cad4",
   "metadata": {},
   "source": [
    "# Probing tsGT code functionality\n",
    "This notebook contains some fundamental tests for checking the functionality of the tsGT model code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368916a-88d7-4541-b44c-d7de4601113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('code')\n",
    "\n",
    "import datasets as ds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30e07f-9bd1-4cad-98d2-305c1af32fd7",
   "metadata": {},
   "source": [
    "## First, let's get an idea of how the Dataset is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16272fa4-36da-45cd-9346-f9754d43306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/synthetic_datasets_segments.pkl', 'rb') as file:\n",
    "    samples = pickle.load(file)\n",
    "samples[\"dgp_dataset\"][\"samples\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e551f67-9056-44de-8266-3908bd26bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_df(samples, start_date='2018-01-01 00:00:00', freq='1h'):\n",
    "    \" Utility function to map a dataset to a dataframe. Covariates here don't matter and are dropped during training.\"\n",
    "    n_signals, length = samples.shape\n",
    "    df = pd.DataFrame(samples.T)\n",
    "    df.index = pd.date_range(start_date, periods=length, freq=freq)\n",
    "    df.index.name = 'date'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964c3a5-e23c-41f3-be5b-ab3d639223c4",
   "metadata": {},
   "source": [
    "## datasets.py\n",
    "The relevant parameters here are:\n",
    "- series_length: this defines the length of the slices of the dataset, for example let the dataset consist of [n_series, length]. Then during training we take random samples of [batch_size, start_idx:end_idx] where end_idx - start_idx = 256\n",
    "- train_window: this denotes the length of the series where we can get training examples from. If eval_window>0 this means that the evaluation window is situated at the end of the series and so train window has shape [n_series, :train_window]. If eval_window=0 then the evaluation window takes up the first series_length points in the series.\n",
    "- eval_window: this defines the window over which loss is to be calculated. eval_window depends on the length of eval_data which is always equal to series_length. For example, if series_length=256 and eval_window = 128. Then eval_data=256. During validation loss calculation the loss is calculated over eval_data using teacher forcing. If eval_window>0 then the loss is calculated only on the 128 points corresponding to eval_window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245e75d-2217-43fc-984a-db2c2050af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy dataset where the first 896 points are 1s and the rest 0s, testing the split.\n",
    "data = np.ones((1024, 1024)) \n",
    "data[896:] = 0\n",
    "df = pd.DataFrame(data, index=pd.date_range('2018-01-01', periods=1024, freq='h'))\n",
    "dataset = ds.Dataset(\n",
    "    data_full=ds.DataCollection(data_loader=lambda: df),\n",
    "    series_length=256,\n",
    "    start_date='2018-01-01',\n",
    "    train_window=896,\n",
    "    eval_window=128)\n",
    "assert dataset.train_data.shape == (1024, 896)\n",
    "assert dataset.eval_data.shape == (1024, 256)\n",
    "assert dataset.eval_horizon == 128\n",
    "assert dataset.train_data.shape == (1024, 896)\n",
    "assert dataset.eval_data.shape == (1024, 256)\n",
    "assert np.all(dataset.train_data == 1)\n",
    "assert np.all(dataset.eval_data[:, :128] == 1) # first 128 points of eval come from the 1s since they are overlapping with the training window\n",
    "assert np.all(dataset.eval_data[:, 128:] == 0) # last 128 cols of eval come from the 0s region\n",
    "assert dataset.eval_horizon == 128\n",
    "assert dataset.cov.shape == (1152,3) # This is because it has a shape of [training_window + eval_window, 3] and since they overlap at 128 points 1024+128 = 1152\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f057bb-7012-4d26-ba00-3fdeb6d5b746",
   "metadata": {},
   "source": [
    "## index_streams.py\n",
    "This is essentially just a supporting package for getting indexes and slices for the input iterables. Not much to test here other than sanity checks. (Note: only relevant bits of code here are the ones that randomly sample eval and train indexes. Everything else is not used. For example, the code provides methods for extracting the full index streams but this is deterministic sampling.) The two relevant functions here are:\n",
    "- create_train_index_stream: this creates the index stream for training examples, so it provides us with a generator of (series_index, slice_start, slice_stop). It chooses a random series and a random starting point. The only relevant parameter here is weighted_sampling:Bool which is kept false. This parameter helps mitigate sampling from series/time points that are 0 (for example seasonal data) but our data does not reflect these trends.\n",
    "- create_eval_index_stream: similar to the above only for the eval index stream, the difference is that we get the whole length and so slice_stop=None. The relevant parameter here is full_eval which is false by default. This makes it so that we evaluate on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e8259-ebc8-4375-a0a2-4d55699ec5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from index_streams import (\n",
    "    create_train_index_stream,\n",
    "    create_eval_index_stream,\n",
    "    create_random_eval_index_stream,\n",
    "    create_uniform_train_index_stream\n",
    ")\n",
    "# Dummy dataset for testing, again...\n",
    "dataset = np.random.randn(5, 100)\n",
    "series_length = 20\n",
    "np.random.seed(0)\n",
    "stream = create_uniform_train_index_stream(dataset, series_length)\n",
    "for i in range(20):\n",
    "    series_idx, slice_start, slice_stop = next(stream)\n",
    "    assert slice_stop - slice_start == series_length\n",
    "    assert 0 <= series_idx < dataset.shape[0]\n",
    "    assert 0 <= slice_start < dataset.shape[1]\n",
    "    assert slice_stop <= dataset.shape[1]\n",
    "np.random.seed(0)\n",
    "stream = create_random_eval_index_stream(dataset, series_length)\n",
    "for i in range(20):\n",
    "    series_idx, slice_start, slice_stop = next(stream)\n",
    "    assert 0 <= series_idx < dataset.shape[0]\n",
    "    assert slice_start == -series_length\n",
    "    assert slice_stop is None\n",
    "    sliced = dataset[series_idx, slice_start:slice_stop]\n",
    "    assert sliced.shape[0] == series_length\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2584da-2210-4bb2-b495-a846dd6db4c3",
   "metadata": {},
   "source": [
    "## inputs.py\n",
    "Using the above index and slice streams, the inputs script now lets us extract inputs to be used for training/eval. The main use of this module is CreateInputs which takes the index streams above and returns actual train and eval streams with train and eval data. A very important variable here is mask. This determines on which parts of the sequence the loss is calculated. For the train sequences the mask is always 1. During eval, the mask is always 1 for eval_horizon. For example, if eval_horizon = 0 and series_length=256 then the mask consists of 256 1s. If eval_horizon 128 and series_length = 256 then the mask has 128 0s followed by 128 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83538a-575c-43c3-9d62-274ed154b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ones((1024, 1024)) \n",
    "data[896:] = 0\n",
    "df = pd.DataFrame(data, index=pd.date_range('2018-01-01', periods=1024, freq='h'))\n",
    "dataset = ds.Dataset(\n",
    "    data_full=ds.DataCollection(data_loader=lambda: df),\n",
    "    series_length=256,\n",
    "    start_date='2018-01-01',\n",
    "    train_window=896,\n",
    "    eval_window=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fcc38-41ee-4bf6-97d0-a4962150bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from index_streams import create_train_index_stream, create_eval_index_stream\n",
    "from inputs import slice_stream, minibatch_stream, CreateInputs\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_stream, eval_stream = CreateInputs(dataset=dataset, batch_size=16, series_length=256,\n",
    "            weighted_sampling=False, full_eval=False, traxify=False)\n",
    "train_batch = next(train_stream(None))\n",
    "eval_batch = next(eval_stream(None))\n",
    "series, inp, target, mask = train_batch\n",
    "eval_series, eval_inp, eval_target, eval_mask = eval_batch\n",
    "assert series.shape == (16, 256)\n",
    "assert inp.shape[0] == 16\n",
    "assert mask.shape == (16, 256)\n",
    "assert np.array_equal(series, target) # This is basically the same series anyways for now, shift comes later for teacher forcing.\n",
    "assert np.all(series == 1)\n",
    "assert np.all(mask == 1) # All 1s since we calculate loss over the whol` hrg,e train series.\n",
    "assert np.all(eval_mask[:, :128] == 0) # This is the overlapping section, mask is 0 here.\n",
    "assert np.all(eval_mask[:, 128:] == 1) # This is where loss is calculated\n",
    "assert np.all(eval_series[:, :128] == 1)\n",
    "assert np.all(eval_series[:, 128:] == 0)\n",
    "assert np.array_equal(eval_series, eval_target)\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7782f-331f-4454-bd64-aeb06f79333c",
   "metadata": {},
   "source": [
    "## normalization.py\n",
    "\n",
    "This module creates the first necessary step of the data preparation process by normalising the data according to the method laid out in the paper. Here it is necessary to essentially test that everything works correctly with known data. An important factor here is also the mask meaning that any positions where mask=0 are excluded from calculating the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93201ac-17fd-4c1f-847b-140e6cb538b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictors.normalization import PerTsNormalizer\n",
    "import numpy as np\n",
    "normalizer = PerTsNormalizer(regularizer=0.001)\n",
    "data = np.array([\n",
    "    [1.0, 1.0, 1.0, 1.0],   # absmean = 1.0, scaling = 1.001\n",
    "    [2.0, 0.0, 2.0, 0.0],   # absmean = 1.0, scaling = 1.001\n",
    "])\n",
    "scaled_data, parameters, mask_mod = normalizer.normalize(data)\n",
    "recovered = normalizer.denormalize(scaled_data, parameters)\n",
    "zs = np.array([[0.0, 0.0, 0.0, 0.0]])\n",
    "scaled_zs, par_zs, a = normalizer.normalize(zs)\n",
    "neg = np.array([[-3.0, 3.0, -3.0, 3.0]])\n",
    "scaled_neg, b, c = normalizer.normalize(negatives)\n",
    "assert np.allclose(parameters.scaling_factor, [[1.001], [1.001]])\n",
    "assert np.allclose(recovered, data)\n",
    "assert np.allclose(par_zs.scaling_factor, [[0.001]])\n",
    "assert np.allclose(scaled_zs, 0.0)\n",
    "assert np.allclose(scaled_neg, [[-3.0/3.001, 3.0/3.001, -3.0/3.001, 3.0/3.001]])\n",
    "# this is now basically testing the mask situation\n",
    "data = np.array([[1.0, 1.0, 10.0, 10.0]])\n",
    "mask = np.array([[0.0, 0.0, 1.0, 1.0]]) # Here the last two are 1, this would be when eval_horizon = 2 but series_length = 4\n",
    "scaled_with_mask, params_mask, _ = normalizer.normalize(data, mask=mask)\n",
    "scaled_no_mask, params_no_mask, _ = normalizer.normalize(data, mask=None)\n",
    "assert np.allclose(params_mask.scaling_factor, [[10.001]])\n",
    "assert np.allclose(scaled_with_mask, [[1.0/10.001, 1.0/10.001, 10.0/10.001, 10.0/10.001]])\n",
    "assert np.allclose(params_no_mask.scaling_factor, [[5.501]])\n",
    "assert np.allclose(scaled_no_mask, [[1.0/5.501, 1.0/5.501, 10.0/5.501, 10.0/5.501]])\n",
    "assert np.allclose(normalizer.denormalize(scaled_with_mask, params_mask), data)\n",
    "assert np.allclose(normalizer.denormalize(scaled_no_mask, params_no_mask), data)\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b70c1-f381-40c0-ba89-ba62b7924355",
   "metadata": {},
   "source": [
    "## serializers.py\n",
    "The serializers script is important as it provides the serialization of floats into discrete tokens and the deserialization of tokens to floats. Gym spaces supports the normalization of the values into a set bound. This is used during preprocessing. There are some important parameters here:\n",
    "- vocab_size: This is the base we multiply the floats with. In the paper this is set to 10\n",
    "- precision: This defines how many tokens we represent each float with. For example precision=3 throughout the paper so with vocab_size = 10. We can get 0.143 = 143. These three tokens then represent the float value 0.143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433d788-7da5-4370-807f-7676702cf755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from serializers import BoxSpaceSerializer\n",
    "space = gym.spaces.Box(low=-0.3, high=1.5, shape=())\n",
    "serializer = BoxSpaceSerializer(space, vocab_size=10, precision=3)\n",
    "assert space.shape == ()\n",
    "assert np.isclose(float(space.low), -0.3) # accounting for floating point errors here...\n",
    "assert np.isclose(float(space.high), 1.5)\n",
    "data = np.array([[0.5], [1.5], [-0.3], [0.0], [1.0]])\n",
    "preprocessed = serializer._preprocess(data) # This is basically the mapping to [0,1] and clipping.\n",
    "low, high = -0.3, 1.5\n",
    "assert np.allclose(preprocessed[0], (0.5 - low) / (high - low))\n",
    "assert np.allclose(preprocessed[1], 1.0)\n",
    "assert np.allclose(preprocessed[2], 0.0) \n",
    "assert np.allclose(preprocessed[3], (0.0- low) / (high - low))\n",
    "assert np.allclose(preprocessed[4], (1.0-low) / (high - low))\n",
    "assert np.all(preprocessed >= 0.0)\n",
    "assert np.all(preprocessed <= 1.0)\n",
    "\n",
    "serialized = serializer.serialize(data)\n",
    "deserialized = serializer.deserialize(serialized)\n",
    "assert serialized.shape == (5, 3)\n",
    "assert np.array_equal(serialized[1], [9, 9, 9]) # Upper bound\n",
    "assert np.array_equal(serialized[2], [0, 0, 0]) # Lower bound here.\n",
    "assert np.isclose(float(deserialized[1]), 1.5, atol=0.01)\n",
    "assert np.isclose(float(deserialized[2]), -0.3, atol=0.01)\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac814b0-c1f6-4ef3-ac31-b20a64903dc5",
   "metadata": {},
   "source": [
    "## predictors/inputs.py\n",
    "This code is responsible for creating a trax layer that injects the inputs with information from the time covariates (as well as series id) as provided by the code. Here we just check if the covariates are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd3378-604c-415d-85c6-6b321a793dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictors.inputs import InjectInputs\n",
    "from trax import shapes\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "injection_layer = InjectInputs(input_vocab_sizes=None, d_emb=256)\n",
    "batch_size, seq_len, d_emb, n_covariates = 2, 10, 128, 4\n",
    "context_emb = jnp.ones((batch_size, seq_len, d_emb))\n",
    "covariates = jnp.ones((batch_size, seq_len, n_covariates))\n",
    "inpts = (\n",
    "    shapes.ShapeDtype((batch_size, seq_len, d_emb)),\n",
    "    shapes.ShapeDtype((batch_size, seq_len, n_covariates)),\n",
    ")\n",
    "injection_layer.init(inpts)\n",
    "output = injection_layer((context_emb, covariates))\n",
    "assert output.shape == (batch_size, seq_len, d_emb)\n",
    "assert np.allclose(output, context_emb)\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7f452-2d09-4d3f-b503-ba097e244b22",
   "metadata": {},
   "source": [
    "## attention.py\n",
    "Attention here is pretty standard, RoPE from Roformer is used to calculate the positional encodings. It is important to test the functions for clarity to see if the numerical outputs are consistent with the behavior that is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718071d-5132-44d2-b539-63775a508bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trax.fastmath import numpy as jnp #jnp when working with trax layers\n",
    "from trax import shapes, fastmath\n",
    "from attention import (\n",
    "    rotate_every_two,\n",
    "    calculate_sin_cos_rotary,\n",
    "    DotProductCausalRotaryAttention,\n",
    ")\n",
    "# Rotation function first\n",
    "x = jnp.array([[[1.0, 2.0, 3.0, 4.0]]])  \n",
    "r = rotate_every_two(x)\n",
    "e = jnp.array([[[-2.0, 1.0, -4.0, 3.0]]])\n",
    "assert jnp.allclose(r, e) # So here we should get the needed rotation for applying ROPE\n",
    "x = jnp.ones((2, 5, 8))\n",
    "assert rotate_every_two(x).shape == (2, 5, 8) #Obviously should work\n",
    "x = jnp.array([[[1.0, 2.0, 3.0, 4.0]]])\n",
    "dr = rotate_every_two(rotate_every_two(x))\n",
    "assert jnp.allclose(dr, -x)\n",
    "# Precomputation of sin cos tables\n",
    "r, ctx = 8, 32\n",
    "sin, cos = calculate_sin_cos_rotary(rotary_dim=r, n_ctx=ctx)\n",
    "assert sin.shape == (ctx, r)\n",
    "assert cos.shape == (ctx, r)\n",
    "assert jnp.allclose(sin**2 + cos**2, 1.0, atol=1e-3)\n",
    "assert jnp.allclose(sin[0], 0.0, atol=1e-6)\n",
    "assert jnp.allclose(cos[0], 1.0, atol=1e-6)\n",
    "assert jnp.all(sin >= -1.0) and jnp.all(sin <= 1.0) # Should all be bounded here\n",
    "assert jnp.all(cos >= -1.0) and jnp.all(cos <= 1.0)\n",
    "\n",
    "batch_heads, seq_len, d_head, fraction_to_rotate = 2, 6, 16, 0.25\n",
    "layer = DotProductCausalRotaryAttention(fraction_to_rotate=fraction_to_rotate,dropout=0.0, max_inference_length=64, mode='eval')\n",
    "sig = (\n",
    "    shapes.ShapeDtype((batch_heads, seq_len, d_head)),\n",
    "    shapes.ShapeDtype((batch_heads, seq_len, d_head)),\n",
    "    shapes.ShapeDtype((batch_heads, seq_len, d_head)),\n",
    ")\n",
    "layer.init(sig)\n",
    "np.random.seed(42)\n",
    "q = jnp.array(np.random.randn(batch_heads, seq_len, d_head))\n",
    "k = jnp.array(np.random.randn(batch_heads, seq_len, d_head))\n",
    "v = jnp.array(np.random.randn(batch_heads, seq_len, d_head))\n",
    "out = layer((q, k, v))\n",
    "assert out.shape == (batch_heads, seq_len, d_head)\n",
    "# We can check the causal mask too\n",
    "q2 = q.at[:, -1, :].set(99.0)\n",
    "k2 = k.at[:, -1, :].set(99.0)\n",
    "v2 = v.at[:, -1, :].set(99.0)\n",
    "out2 = layer((q2, k2, v2))\n",
    "assert jnp.allclose(out[:, :-1, :], out2[:, :-1, :], atol=1e-5)\n",
    "#  Check permutation invariance, this should not be true of course.\n",
    "perm = jnp.array([1, 0] + list(range(2, seq_len)))\n",
    "q_perm = q[:, perm, :]\n",
    "k_perm = k[:, perm, :]\n",
    "v_perm = v[:, perm, :]\n",
    "out_perm = layer((q_perm, k_perm, v_perm))\n",
    "assert not jnp.allclose(out, out_perm, atol=1e-3)\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce853878-d167-4f6e-9f7e-db4038ddfe7b",
   "metadata": {},
   "source": [
    "## layers.py\n",
    "This code module contains a lot of utility layers that are not actually utilised, neither in the original experimental setting(when testing the actual tsGT model) nor in the one proposed in the thesis. Essentially the only layer that is utilised here is CausalConv but with a kernel width of 1 which is effectively a linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a7b9a-819a-4388-ac31-386e2c5e68ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trax import shapes\n",
    "from trax.fastmath import numpy as jnp\n",
    "from layers import CausalConv\n",
    "conv = CausalConv(filters=256, kernel_width=1, mode='eval')\n",
    "conv.init(shapes.ShapeDtype((2, 10, 128)))\n",
    "x = jnp.array(np.random.randn(2, 10, 128))\n",
    "out = conv(x)\n",
    "assert out.shape == (2, 10, 256)\n",
    "conv = CausalConv(filters=32, kernel_width=1, mode='eval')\n",
    "conv.init(shapes.ShapeDtype((1, 8, 16)))\n",
    "x = jnp.array(np.random.randn(1, 8, 16).astype(np.float32))\n",
    "out = conv(x)\n",
    "x_mod = x.at[:, 3, :].set(99.0)\n",
    "out_mod = conv(x_mod) # Modifying one position here shold not change a lot\n",
    "assert jnp.allclose(out[:, :3, :], out_mod[:, :3, :],atol=1e-3)\n",
    "assert jnp.allclose(out[:, 4:, :],out_mod[:, 4:, :], atol=1e-3)\n",
    "assert not jnp.allclose(out[:, 3, :], out_mod[:, 3, :],atol=1e-3)\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e314e-996e-4c3f-a1f6-c9fe99669e1d",
   "metadata": {},
   "source": [
    "## models.py\n",
    "This code contains the main transformer body defined using trax layers that is to be used throughout the experiment. The model consists of decoder blocks, feedforward blocks and a causal convolution layer of kernel width 1, essentially a Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93d36f-86ec-40d1-8535-bbc11e18265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trax import layers as tl, shapes\n",
    "from trax.fastmath import numpy as jnp\n",
    "import models\n",
    "\n",
    "def return_body(mode='eval'):\n",
    "    return models.TransformerBody(\n",
    "        d_model=128, d_ff_mul=2, n_layers=4, n_heads=4,\n",
    "        max_len=2048, dropout=0.0, ff_activation=tl.FastGelu, precision=3\n",
    "    )\n",
    "\n",
    "body = return_body()\n",
    "batch, seq_len, d_model = 2, 24, 128\n",
    "sig = shapes.ShapeDtype((batch, seq_len, d_model))\n",
    "body.init(sig)\n",
    "x = jnp.array(np.random.randn(batch, seq_len, d_model).astype(np.float32))\n",
    "out = body(x)\n",
    "assert out.shape == (batch, seq_len, d_model)\n",
    "\n",
    "out1 = body(x)\n",
    "out2 = body(x)\n",
    "assert jnp.allclose(out1, out2, atol=1e-3)\n",
    "\n",
    "perm = jnp.array([1, 0] + list(range(2, seq_len)))\n",
    "x_perm = x[:, perm, :]\n",
    "out_perm = body(x_perm)\n",
    "assert not jnp.allclose(out, out_perm, atol=1e-3)\n",
    "\n",
    "x_alt = jnp.array(np.random.randn(batch, seq_len, d_model).astype(np.float32))\n",
    "out_alt = body(x_alt)\n",
    "assert jnp.allclose(out[:, 0, :], out_alt[:, 0, :], atol=1e-3)\n",
    "\n",
    "# Variable lengths \n",
    "for sl in [12, 48, 768]:\n",
    "    x_var = jnp.array(np.random.randn(1, sl, d_model).astype(np.float32))\n",
    "    out_var = body(x_var)\n",
    "    assert out_var.shape == (1, sl, d_model)\n",
    "\n",
    "# batch independence\n",
    "x_a = jnp.array(np.random.randn(1, 24, d_model).astype(np.float32))\n",
    "x_b = jnp.array(np.random.randn(1, 24, d_model).astype(np.float32))\n",
    "x_batched = jnp.concatenate([x_a, x_b], axis=0)\n",
    "out_a = body(x_a)\n",
    "out_b = body(x_b)  \n",
    "out_batched = body(x_batched)\n",
    "assert jnp.allclose(out_batched[0], out_a[0], atol=1e-3)\n",
    "assert jnp.allclose(out_batched[1], out_b[0], atol=1e-3)\n",
    "\n",
    "x = jnp.array(np.random.randn(1, 24, d_model).astype(np.float32))\n",
    "out = body(x)\n",
    "assert not jnp.allclose(out[:, 1:, :], x[:, :-1, :], atol=1e-3)\n",
    "\n",
    "# Numerical stability\n",
    "x_large = jnp.array(np.random.randn(2, 24, d_model).astype(np.float32)) * 10.0\n",
    "out_large = body(x_large)\n",
    "assert jnp.all(jnp.isfinite(out_large))\n",
    "\n",
    "x_small = jnp.array(np.random.randn(2, 24, d_model).astype(np.float32)) * 1e-6\n",
    "out_small = body(x_small)\n",
    "assert jnp.all(jnp.isfinite(out_small))\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb479029-6f3c-4826-a10e-c6bc20ae46c0",
   "metadata": {},
   "source": [
    "## decoding.py\n",
    "Autoregressive decoding of discretised continuous values. This decoding scheme needs a SerialDecoder model that is available in predictors/serial_predictor. It essentially lets the model handle the discrete digits. Here we just test the functionality of the code with some sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b375b4-31ad-4edb-91e5-852517a0babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from trax import layers as tl, shapes\n",
    "from trax.fastmath import numpy as jnp\n",
    "import models\n",
    "from decoding import autoregressive_sample\n",
    "from predictors.serial_predictor import SerialDecoder\n",
    "from serializers import BoxSpaceSerializer\n",
    "\n",
    "vocab_size, d_model, precision = 10, 64, 3\n",
    "serializer = BoxSpaceSerializer( space=gym.spaces.Box(shape=(), low=0.0, high=10.0),vocab_size=vocab_size,precision=precision)\n",
    "body = models.TransformerBody( d_model=d_model, d_ff_mul=2, n_layers=2, n_heads=2,\n",
    "    max_len=2048, dropout=0.0, ff_activation=tl.FastGelu,precision=precision, mode='predict')\n",
    "\n",
    "model = SerialDecoder(model_body=body, serializer=serializer,d_emb=d_model,input_vocab_sizes=None, mode='predict')\n",
    "sig = (shapes.ShapeDtype((1, 1), dtype=np.int32),shapes.ShapeDtype((1, 1, 0), dtype=np.int32))\n",
    "model.init(sig)\n",
    "init_state = model.state\n",
    "\n",
    "# Just some placeholder sampling for logits\n",
    "def greedy_sample(logits):\n",
    "    return np.argmax(logits, axis=-1)\n",
    "context_len, horizon = 6, 9\n",
    "context = np.random.randint(0, vocab_size, (1, context_len))\n",
    "inputs = np.zeros((1, context_len + horizon, 0))\n",
    "model.state = init_state\n",
    "result = autoregressive_sample(\n",
    "    model=model, sample_fn=greedy_sample,\n",
    "    context=context, inputs=inputs,\n",
    "    batch_size=1, horizon_length=horizon,\n",
    ")\n",
    "assert result.shape == (1, horizon)\n",
    "assert np.all(result >= 0) and np.all(result < vocab_size)\n",
    "# It's actually sensitive to the context\n",
    "model.state = init_state\n",
    "result_a = autoregressive_sample(\n",
    "    model=model, sample_fn=greedy_sample,\n",
    "    context=np.array([[1, 1, 1, 1, 1, 1]], dtype=np.int32),\n",
    "    inputs=inputs, batch_size=1, horizon_length=horizon,\n",
    ")\n",
    "model.state = init_state\n",
    "result_b = autoregressive_sample(\n",
    "    model=model, sample_fn=greedy_sample,\n",
    "    context=np.array([[9, 9, 9, 9, 9, 9]], dtype=np.int32),\n",
    "    inputs=inputs, batch_size=1, horizon_length=horizon,\n",
    ")\n",
    "assert not np.array_equal(result_a, result_b)\n",
    "\n",
    "# Deserialise\n",
    "pred_repr = np.reshape(result, (-1, precision))\n",
    "values = serializer.deserialize(pred_repr)\n",
    "assert np.all(np.isfinite(values))\n",
    "assert values.shape == (horizon // precision,)\n",
    "print(\"All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4875c-ed98-4fee-98ee-60d4877a1efc",
   "metadata": {},
   "source": [
    "## serial_predictor.py\n",
    "This is where the whole pipeline is put together and everything from training to prediction becomes possible. Of course the predictor itself does not have a state so as long as every component is instantiated with the same parameters and the model weights are there (From the decoder mainly) then it is possible to for example reconstruct the predictor used during training for evaluation. Here we can test every aspect of the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6eb46c-cfdf-40ef-81ca-15c7b145bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gin\n",
    "from trax import layers as tl, shapes\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax.rl import serialization_utils as srl_utils\n",
    "import models\n",
    "from predictors.serial_predictor import SerialPredictor, SerialDecoder, SerialTraining\n",
    "from predictors.normalization import Normalizer\n",
    "from serializers import BoxSpaceSerializer\n",
    "from distributions import Categorical\n",
    "gin.clear_config()\n",
    "\n",
    "vocab_size, d_model, precision, significance_decay, low, high, batch_size, context_len, horizon = 10, 64, 3, 0.3, 0.0, 10.0, 2, 24, 8\n",
    "def make_body(mode='eval', precision=3):\n",
    "    return models.TransformerBody(\n",
    "        d_model=d_model, d_ff_mul=2, n_layers=2, n_heads=2,\n",
    "        max_len=2048, dropout=0.0, ff_activation=tl.FastGelu,\n",
    "        precision=precision, mode=mode,\n",
    "    )\n",
    "\n",
    "predictor = SerialPredictor(\n",
    "    model_body_fn=make_body,\n",
    "    d_in=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    precision=precision,\n",
    "    significance_decay=significance_decay, # this is for weighing the significance of digits to the weight contribution\n",
    "    low=low,\n",
    "    high=high,\n",
    "    accelerate_predict_model=False, # This is for testing\n",
    "    normalization_regularizer=0.001)\n",
    "series_len = context_len + horizon\n",
    "train_model = predictor.make_train_eval_model(mode='train')\n",
    "sig = (\n",
    "    shapes.ShapeDtype((batch_size, series_len), dtype=np.float32),\n",
    "    shapes.ShapeDtype((batch_size, series_len, 0), dtype=np.int32),\n",
    "    shapes.ShapeDtype((batch_size, series_len), dtype=np.float32),\n",
    "    shapes.ShapeDtype((batch_size, series_len), dtype=np.float32),\n",
    ")\n",
    "train_model.init(sig)\n",
    "train_weights = train_model.weights\n",
    "np.random.seed(67)\n",
    "context = np.random.rand(batch_size, context_len) * 5.0\n",
    "inputs = np.zeros((batch_size, context_len + horizon, 0))\n",
    "\n",
    "pred = predictor.predict(\n",
    "    weights=train_weights,\n",
    "    context=context,\n",
    "    inputs=inputs,\n",
    "    horizon_length=horizon)\n",
    "assert pred.shape == (batch_size, horizon)\n",
    "assert np.all(np.isfinite(pred))\n",
    "original_sample = predictor._categorical.sample\n",
    "predictor._categorical.sample = lambda logits, **kwargs: np.argmax(logits, axis=-1) # just do greedy here, like we did before\n",
    "pred1 = predictor.predict(\n",
    "    weights=train_weights, context=context,\n",
    "    inputs=inputs, horizon_length=horizon,\n",
    ")\n",
    "pred2 = predictor.predict(\n",
    "    weights=train_weights, context=context,\n",
    "    inputs=inputs, horizon_length=horizon,\n",
    ")\n",
    "assert np.allclose(pred1, pred2, atol=1e-6)\n",
    "predictor._categorical.sample = original_sample\n",
    "predictor._categorical.sample = lambda logits, **kwargs: np.argmax(logits, axis=-1)\n",
    "ctx_low = np.ones((2, context_len)) * 0.1\n",
    "ctx_high = np.ones((2, context_len)) * 9.0\n",
    "inputs_single = np.zeros((2, context_len + horizon, 0))\n",
    "pred_low = predictor.predict(\n",
    "    weights=train_weights, context=ctx_low,\n",
    "    inputs=inputs_single, horizon_length=horizon,\n",
    ")\n",
    "pred_high = predictor.predict(\n",
    "    weights=train_weights, context=ctx_high,\n",
    "    inputs=inputs_single, horizon_length=horizon,\n",
    ")\n",
    "assert not np.allclose(pred_low, pred_high, atol=1e-3)\n",
    "predictor._categorical.sample = original_sample\n",
    "\n",
    "predictor._categorical.sample = lambda logits, **kwargs: np.argmax(logits, axis=-1) \n",
    "for h in [4, 12, 24]:\n",
    "    inp = np.zeros((2, context_len + h, 0))\n",
    "    p = predictor.predict(\n",
    "        weights=train_weights, context=ctx_low,\n",
    "        inputs=inp, horizon_length=h,\n",
    "    )\n",
    "    assert p.shape == (2, h)\n",
    "predictor._categorical.sample = original_sample\n",
    "normalizer = predictor._normalizer\n",
    "series = np.random.rand(2, 50)* 8.0\n",
    "\n",
    "norm_series, params, _ = normalizer.normalize(series)\n",
    "denorm_series = normalizer.denormalize(norm_series, params)\n",
    "assert np.allclose(series, denorm_series, atol=1e-3)\n",
    "serializer = predictor._serializer\n",
    "norm_data, _, _ = normalizer.normalize(context)\n",
    "serialized = serializer.serialize(norm_data)\n",
    "assert serialized.shape == (batch_size, context_len * precision)\n",
    "assert np.all(serialized >= 0) and np.all(serialized < vocab_size)\n",
    "\n",
    "# Deserialize back into cont values\n",
    "reshaped = np.reshape(serialized, (-1, precision))\n",
    "deserialized = serializer.deserialize(reshaped)\n",
    "deserialized = np.reshape(deserialized, (batch_size, context_len))\n",
    "assert np.allclose(norm_data, deserialized, atol=0.01)\n",
    "\n",
    "train_model = predictor.make_train_eval_model(mode='train')\n",
    "series_len = context_len + horizon\n",
    "series = np.random.rand(batch_size, series_len) * 5.0\n",
    "inputs_train = np.zeros((batch_size, series_len, 0))\n",
    "target = series.copy()\n",
    "mask = np.zeros((batch_size, series_len))\n",
    "mask[:, -horizon:] = 1.0\n",
    "sig = (\n",
    "    shapes.ShapeDtype(series.shape, dtype=np.float32),\n",
    "    shapes.ShapeDtype(inputs_train.shape, dtype=np.int32),\n",
    "    shapes.ShapeDtype(target.shape, dtype=np.float32),\n",
    "    shapes.ShapeDtype(mask.shape, dtype=np.float32),\n",
    ")\n",
    "train_model.init(sig)\n",
    "output = train_model((series, inputs_train, target, mask))\n",
    "logits, target_repr, weights = output\n",
    "\n",
    "assert logits.shape[-1] == vocab_size\n",
    "assert target_repr.dtype in (np.int32, np.int64, jnp.int32, jnp.int64) # should just be some integer here\n",
    "assert np.all(weights >= 0)\n",
    "\n",
    "\n",
    "sig_map = serializer.significance_map \n",
    "expected_weights_pattern = significance_decay ** sig_map\n",
    "assert np.isclose(significance_decay ** 0, 1.0)\n",
    "assert np.isclose(significance_decay ** 1, 0.3)\n",
    "assert np.isclose(significance_decay ** 2, 0.09)\n",
    "assert expected_weights_pattern[0] > expected_weights_pattern[1] > expected_weights_pattern[2]\n",
    "\n",
    "# we also test WeightedCrossEntropyLoss\n",
    "loss_layer = predictor.make_loss()\n",
    "loss_layer.init(\n",
    "    (\n",
    "        shapes.ShapeDtype(logits.shape, dtype=np.float32),\n",
    "        shapes.ShapeDtype(target_repr.shape, dtype=target_repr.dtype),\n",
    "        shapes.ShapeDtype(weights.shape, dtype=np.float32),\n",
    "    )\n",
    ")\n",
    "loss = loss_layer((logits, target_repr, weights))\n",
    "assert np.isscalar(loss) or loss.shape == ()\n",
    "assert np.isfinite(float(loss))\n",
    "assert float(loss) > 0\n",
    "\n",
    "# We can also check the eval model\n",
    "eval_model = predictor.make_train_eval_model(mode='eval')\n",
    "eval_model.init(sig)\n",
    "output_eval = eval_model((series, inputs_train, target, mask))\n",
    "logits_eval, target_repr_eval, weights_eval = output_eval\n",
    "assert logits_eval.shape[-1] == vocab_size\n",
    "assert np.all(np.isfinite(logits_eval))\n",
    "assert not np.allclose(logits, logits_eval, atol=1e-3)\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e4f16-7f44-4868-961d-aaedc9aaffb4",
   "metadata": {},
   "source": [
    "## Training tests\n",
    "These tests are meant to probe the functionality of trax methods for training, it is important to test the compatibility of the library with the code as, for example, trainer.py essentially just uses the trax training loop method for running training. This makes the training a bit more monolithic as everything is handled in trax internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46850e9e-1bdc-4f18-a8e2-332c88c89c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import gin\n",
    "from trax import layers as tl, shapes, optimizers\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax.supervised import training, lr_schedules\n",
    "import models\n",
    "from predictors.serial_predictor import SerialPredictor\n",
    "import os\n",
    "gin.clear_config()\n",
    "vocab_size, d_model, precision, batch_size, series_len, horizon = 10, 64, 3, 2, 32, 8\n",
    "def make_body(mode='eval', precision=3):\n",
    "    return models.TransformerBody(\n",
    "        d_model=d_model, d_ff_mul=2, n_layers=2, n_heads=2,\n",
    "        max_len=2048, dropout=0.0, ff_activation=tl.FastGelu,\n",
    "        precision=precision, mode=mode,\n",
    "    )\n",
    "# Accelerate should be working, training is slow but this isn't a problem caused by accelerate...\n",
    "predictor = SerialPredictor(\n",
    "    model_body_fn=make_body,\n",
    "    d_in=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    precision=precision,\n",
    "    significance_decay=0.3,\n",
    "    low=0.0, high=10.0,\n",
    "    accelerate_predict_model=False,\n",
    "    normalization_regularizer=0.001)\n",
    "train_model = predictor.make_train_eval_model(mode='train')\n",
    "acc_model = tl.Accelerate(train_model)\n",
    "series = np.random.rand(batch_size, series_len) * 5.0\n",
    "inputs_train = np.zeros((batch_size, series_len, 0))\n",
    "target = series.copy()\n",
    "mask = np.ones((batch_size, series_len))\n",
    "sig = (\n",
    "    shapes.ShapeDtype(series.shape, dtype=np.float32),\n",
    "    shapes.ShapeDtype(inputs_train.shape, dtype=np.int32),\n",
    "    shapes.ShapeDtype(target.shape, dtype=np.float32),\n",
    "    shapes.ShapeDtype(mask.shape, dtype=np.float32),\n",
    ")\n",
    "acc_model.init(sig)\n",
    "out = acc_model((series, inputs_train, target, mask))\n",
    "logits, target_repr, weights = out\n",
    "assert logits.shape[-1] == vocab_size\n",
    "assert np.all(np.isfinite(logits))\n",
    "\n",
    "optimizer = optimizers.Adam() # Trax optimizer should be loading\n",
    "assert optimizer is not None\n",
    "\n",
    "# This is the LR schedule in the paper, shouldnt be a problem\n",
    "schedule = lr_schedules.multifactor(\n",
    "    constant=0.03,\n",
    "    factors='constant * linear_warmup * rsqrt_decay',\n",
    "    warmup_steps=1000,\n",
    ")\n",
    "assert schedule(0) >= 0\n",
    "assert schedule(500) > schedule(0)\n",
    "assert schedule(1000) >= schedule(500)\n",
    "assert schedule(5000)< schedule(1000)\n",
    "assert all(np.isfinite([schedule(0), schedule(500), schedule(1000), schedule(5000)]))\n",
    "\n",
    "def example_stream():\n",
    "    while True:\n",
    "        s = np.random.rand(batch_size, series_len) * 5.0\n",
    "        i = np.zeros((batch_size, series_len, 0))\n",
    "        t = s.copy()\n",
    "        m = np.ones((batch_size, series_len))\n",
    "        yield (s, i, t, m)\n",
    "stream = example_stream()\n",
    "batch = next(stream)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    example_stream(),\n",
    "    loss_layer=predictor.make_loss(),\n",
    "    optimizer=optimizers.Adam(),\n",
    "    lr_schedule=lr_schedules.multifactor(\n",
    "        constant=0.03,\n",
    "        factors='constant * linear_warmup * rsqrt_decay',\n",
    "        warmup_steps=100,\n",
    "    ),\n",
    "    n_steps_per_checkpoint=50,\n",
    ")\n",
    "assert train_task is not None # make sure it loads correctly.\n",
    "\n",
    "# this is basically just copied from the training method\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    train_model = tl.Accelerate(predictor.make_train_eval_model(mode='train'))\n",
    "    eval_model = tl.Accelerate(predictor.make_train_eval_model(mode='eval'))\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        example_stream(),\n",
    "        metrics=[predictor.make_loss()],\n",
    "        metric_names=['loss'],\n",
    "        n_eval_batches=2,\n",
    "    )\n",
    "\n",
    "    loop = training.Loop(\n",
    "        model=train_model,\n",
    "        tasks=[training.TrainTask(\n",
    "            example_stream(),\n",
    "            loss_layer=predictor.make_loss(),\n",
    "            optimizer=optimizers.Adam(),\n",
    "            lr_schedule=lr_schedules.multifactor(\n",
    "                constant=0.03,\n",
    "                factors='constant * linear_warmup * rsqrt_decay',\n",
    "                warmup_steps=100,\n",
    "            ),\n",
    "            n_steps_per_checkpoint=10,\n",
    "        )],\n",
    "        eval_model=eval_model,\n",
    "        eval_tasks=[eval_task],\n",
    "        output_dir=tmpdir,\n",
    "        n_devices=1,\n",
    "        checkpoint_at=lambda step: False,\n",
    "        permanent_checkpoint_at=lambda step: False,\n",
    "    )\n",
    "    loop.run(5) # make sure it works\n",
    "    assert loop.step == 5\n",
    "print(\"All tests passed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
