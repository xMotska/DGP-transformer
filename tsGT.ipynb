{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tsGT Training\n",
    "\n",
    "This script contains all necessary code for training the tsGT models used in the experiments. After training, a prediction procedure for all models is carried out in order to get all necessary results that are to be used in estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('code')\n",
    "\n",
    "import predictors\n",
    "import inputs\n",
    "import models\n",
    "import trainer\n",
    "import datasets as ds\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from trax import optimizers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "import trax\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "First, the datasets that are to be used for training need to be imported. These datasets are created in the \"Synthetic Data Generation\" notebook and amount to a dictionary containing the three datasets. The created synthetic datasets can be found at \"data/synthetic_datasets.pkl\" and were created in another environment to avoid conflicting libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/synthetic_datasets_segments.pkl', 'rb') as file:\n",
    "    samples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_do_df(samples, start_date='2018-01-01 00:00:00', freq='1h'):\n",
    "    \" Utility function to map a dataset to a dataframe. Covariates here don't matter and are dropped during training.\"\n",
    "    n_signals, length = samples.shape\n",
    "    df = pd.DataFrame(samples.T)\n",
    "    df.index = pd.date_range(start_date, periods=length, freq=freq)\n",
    "    df.index.name = 'date'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is carried out across fixed seeds in order to keep results reproducible. 10 models are trained to account for variability during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization and clipping\n",
    "As the data gets normalized first and then mapped to [0,1], we first need to figure out the minimum and maximum representable values in the data. To do, since we normalize per time series we can use the following code. This tells us the low and high parameters in the predictor, which represent the maximun range of normalized parameters we can expect. We calculate this on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictors.normalization import PerTsNormalizer\n",
    "normalizer = PerTsNormalizer(regularizer=0.001) # same value as the normalization_regularizer passed to the predictor\n",
    "norm_data, _, _ = normalizer.normalize(samples[\"dgp_dataset\"][\"samples\"])\n",
    "print(f\"Normalized range: [{norm_data.min():.2f}, {norm_data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import lr_schedules\n",
    "import functools\n",
    "\n",
    "lr_schedule = functools.partial(\n",
    "    lr_schedules.multifactor,\n",
    "    constant=0.03,\n",
    "    factors='constant * linear_warmup * rsqrt_decay',\n",
    "    warmup_steps=1000\n",
    ")\n",
    "seeds = [3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049]\n",
    "for seed in seeds:\n",
    "    # Use 80% for training\n",
    "    signals = samples[\"dgp_dataset\"][\"samples\"]\n",
    "    signal_length = 1024\n",
    "    n_signals = int(0.8*len(signals))\n",
    "    n_steps = 5000\n",
    "    start_date = pd.Timestamp('2018-01-01 00:00:00')\n",
    "    freq = '1h'\n",
    "    train_window = 896     \n",
    "    eval_window = 128         \n",
    "    series_length = 256\n",
    "    batch_size = 16\n",
    "\n",
    "    # create a dataframe for the data\n",
    "    df = data_do_df(signals[:n_signals], start_date=start_date, freq=freq)\n",
    "\n",
    "    dataset = ds.Dataset(\n",
    "        data_full=ds.DataCollection(data_loader=lambda: df),\n",
    "        start_date=start_date,\n",
    "        train_window=train_window,\n",
    "        eval_window=eval_window,\n",
    "        series_length=series_length,\n",
    "    )\n",
    "    inputs_iterable = functools.partial(\n",
    "        inputs.CreateInputs,\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        series_length=series_length,\n",
    "        weighted_sampling=False,\n",
    "        traxify=True\n",
    "    )\n",
    "    model_body = functools.partial(\n",
    "    models.TransformerBody,\n",
    "        d_model=128,     \n",
    "        d_ff_mul=2,\n",
    "        n_layers=4,         \n",
    "        n_heads=4,\n",
    "        max_len=2048,\n",
    "        dropout=0.1,\n",
    "        ff_activation=trax.layers.FastGelu,\n",
    "    )\n",
    "    predictor_class = functools.partial(\n",
    "        predictors.SerialPredictor,\n",
    "        d_in=128,           \n",
    "        vocab_size=10,      \n",
    "        precision=3,        \n",
    "        significance_decay=0.3,  \n",
    "        low=-6.0,\n",
    "        high=6.0,\n",
    "        normalization=\"per_ts\",\n",
    "        normalization_regularizer=0.001\n",
    "    )\n",
    "    loop, predictor = trainer.train(\n",
    "        output_dir=f'./out/training_result/segmented{seed}_2',\n",
    "        inputs=functools.partial(inputs_iterable, full_eval=False),\n",
    "        model_body=model_body,\n",
    "        predictor_class=predictor_class,\n",
    "        optimizer=optimizers.Adam,\n",
    "        lr_schedule=lr_schedule,\n",
    "        n_steps=n_steps,     \n",
    "        eval_every=100,\n",
    "        n_eval_batches=32,\n",
    "        calc_eval_loss=True,\n",
    "        seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "After training, we are able to reconstruct the predictor and pass the weights back into it to get the predictions conditioned on a predefined context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function to load model weights from the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import jax\n",
    "import io\n",
    "import pickle\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_structured_weights(weights_path):\n",
    "    \"\"\"Trax checkpoints save flattened weights, therefore we need to reconstruct their structure. To make things easier, we save the weights of a model\n",
    "    with the same hyperparameters and use its weights to reconstruct the weight structure. This file is provided with the code.\"\"\"\n",
    "    \n",
    "    with gzip.open(weights_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "    # First get the flattened arrays from the gz file.\n",
    "    buffer = io.BytesIO(content)\n",
    "    arrays = []\n",
    "    while buffer.tell() < len(content):\n",
    "        try:\n",
    "            arr = np.load(buffer, allow_pickle=True)\n",
    "            arrays.append(arr)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except EOFError:\n",
    "            break\n",
    "    # Now get the nested weight structure.\n",
    "    with gzip.open(\"eval_model_weights.pkl.gz\", \"rb\") as f:\n",
    "        weights = pickle.load(f)\n",
    "    # use jax's tree utils, useful...\n",
    "    structure = jax.tree_util.tree_structure(weights)\n",
    "    reconstructed_weights = jax.tree_util.tree_unflatten(structure, arrays)\n",
    "    if jax.tree_util.tree_structure(reconstructed_weights) == structure:\n",
    "        print(\"Weight structure reconstructed succesfully\")\n",
    "    return reconstructed_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to reconstruct the predictor and a model that needs to be passed to the predictor. These can be rebuilt using the same hyperparameters as they do not contain any relevant internal states. Instead, we will pass the weights to the predictor later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_predictor():\n",
    "    \"\"\"Reconstructs the predictor with the same settings used during training.\"\"\"\n",
    "    model_body = functools.partial(\n",
    "        models.TransformerBody,\n",
    "            d_model=128,     \n",
    "            d_ff_mul=2,\n",
    "            n_layers=4,         \n",
    "            n_heads=4,\n",
    "            max_len=2048,\n",
    "            dropout=0.1,\n",
    "            ff_activation=trax.layers.FastGelu,\n",
    "        )\n",
    "    predictor = predictors.SerialPredictor(\n",
    "        model_body_fn=model_body,\n",
    "        d_in=128,\n",
    "        vocab_size=10,\n",
    "        precision=3,\n",
    "        significance_decay=0.3,\n",
    "        low=-6.0,\n",
    "        high=6.0,\n",
    "        normalization=\"per_ts\",\n",
    "        normalization_regularizer=0.001\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(context, weights, pred_len, predictor, batch_size=16):\n",
    "    \"\"\"Use the predictor and a context of [n_series, context] to make predictions.\"\"\"\n",
    "    n_series = context.shape[0]\n",
    "    all_predictions = []\n",
    "    \n",
    "    for start in range(0, n_series, batch_size):\n",
    "        end = min(start + batch_size, n_series)\n",
    "        batch_context = context[start:end]\n",
    "        actual_size = batch_context.shape[0] # Remember if batch size <16\n",
    "        \n",
    "        # Since predictor needs to be initialised with batch_size, if we run out of batches of 16 just pad and then drop.\n",
    "        if actual_size < batch_size:\n",
    "            padding = np.zeros((batch_size - actual_size, batch_context.shape[1]))\n",
    "            batch_context = np.concatenate([batch_context, padding], axis=0)\n",
    "        \n",
    "        context_length = batch_context.shape[1]\n",
    "        total_length = context_length + pred_len\n",
    "        inputs = np.zeros((batch_size, total_length, 1), dtype=np.int32)\n",
    "        \n",
    "        predictions = predictor.predict(\n",
    "            weights=weights, \n",
    "            context=batch_context, \n",
    "            horizon_length=pred_len, \n",
    "            inputs=inputs\n",
    "        )\n",
    "        full_series = np.concatenate([batch_context, predictions], axis=1)\n",
    "        \n",
    "        # remove the padded series here, we just need them to initialise the predictor.\n",
    "        full_series = full_series[:actual_size]\n",
    "        all_predictions.append(full_series)\n",
    "    \n",
    "    return np.concatenate(all_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We can now create and save the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/synthetic_datasets_segments.pkl', 'rb') as file:\n",
    "    samples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-segment predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [3040 + i for i in range(10)]\n",
    "results = {}\n",
    "for seed in seeds:\n",
    "    results[seed] = {}\n",
    "    # Load the model's weights based on seed.\n",
    "    weights = load_structured_weights(f\"out/training_result/segmented{seed}_2/model_5000.weights.npy.gz\") # only grab the latest state\n",
    "    predictor = reconstruct_predictor()\n",
    "    # Get the last 205 sequences from the dataset.\n",
    "    signals = samples[\"dgp_dataset\"][\"samples\"][819:,]\n",
    "    for i in range(4):\n",
    "        seg_start = i*256\n",
    "        seg_end = seg_start + 256\n",
    "        context_end = seg_start + 128\n",
    "        contexts = signals[:, seg_start:context_end]\n",
    "\n",
    "        # Predict the next 128 to make sure we stay within the segment\n",
    "        series = make_predictions(contexts, weights, 128, predictor)\n",
    "        results[seed][f\"segment{i+1}\"] = series\n",
    "        print(f\"Finished with {seed}, segment {i+1}\")\n",
    "        \n",
    "with open(\"out/predictions/in_segment.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-segment predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [3040 + i for i in range(10)]\n",
    "results = {}\n",
    "for seed in seeds:\n",
    "    results[seed] = {}\n",
    "    # Load the model's weights based on seed.\n",
    "    weights = load_structured_weights(f\"out/training_result/segmented{seed}_2/model_5000.weights.npy.gz\") # only grab the latest state\n",
    "    predictor = reconstruct_predictor()\n",
    "    # Get the last 205 sequences from the dataset.\n",
    "    signals = samples[\"dgp_dataset\"][\"samples\"][819:,]\n",
    "    for i in range(3):\n",
    "        seg_start = i*256\n",
    "        seg_end = seg_start + 256\n",
    "        context_end = seg_start + 256\n",
    "        contexts = signals[:, seg_start:context_end]\n",
    "\n",
    "        # Predict the next 256 to make sure we stay within the segment\n",
    "        series = make_predictions(contexts, weights, 256, predictor)\n",
    "        results[seed][f\"segment{i+1}\"] = series\n",
    "        print(f\"Finished with {seed}, segment {i+1}\")\n",
    "        \n",
    "with open(\"out/predictions/cross_segment.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/val loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport after training...\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import trax\n",
    "\n",
    "seeds = [3040 + i for i in range(10)]\n",
    "all_train_losses = []\n",
    "all_eval_losses = []\n",
    "for seed in seeds:\n",
    "    checkpoint_file = f'out/training_result/segmented{seed}_2/model_5000.pkl.gz'\n",
    "    # Load the model checkpoint to get the losses, these are in the model_{steps}.pkl.gz files...\n",
    "    with gzip.open(checkpoint_file, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    \n",
    "    # These are basically stored in a custom trax class which has everything we need.\n",
    "    train_losses = checkpoint['history']['_values']['train']['metrics/WeightedCategoryCrossEntropy']\n",
    "    eval_losses = checkpoint['history']['_values']['eval']['metrics/WeightedCategoryCrossEntropy']\n",
    "    # We can make these into a normal dict for ease of access.\n",
    "    train_dict = {step: float(loss) for step, loss in train_losses}\n",
    "    eval_dict = {step: float(loss) for step, loss in eval_losses}\n",
    "    all_train_losses.append(train_dict)\n",
    "    all_eval_losses.append(eval_dict)\n",
    "\n",
    "train_steps = sorted(all_train_losses[0].keys())\n",
    "eval_steps = sorted(all_eval_losses[0].keys())\n",
    "# Compute mean and std for train, will help with comparisons.\n",
    "train_mean = []\n",
    "train_std = []\n",
    "for step in train_steps:\n",
    "    values = [d[step] for d in all_train_losses if step in d]\n",
    "    train_mean.append(np.mean(values))\n",
    "    train_std.append(np.std(values))\n",
    "train_mean = np.array(train_mean)\n",
    "train_std = np.array(train_std)\n",
    "# Compute mean and std for eval\n",
    "eval_mean = []\n",
    "eval_std = []\n",
    "for step in eval_steps:\n",
    "    values = [d[step] for d in all_eval_losses if step in d]\n",
    "    eval_mean.append(np.mean(values))\n",
    "    eval_std.append(np.std(values))\n",
    "eval_mean = np.array(eval_mean)\n",
    "eval_std = np.array(eval_std)\n",
    "# We can plot these now.\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# First the training loss\n",
    "ax.plot(train_steps, train_mean, label='Train Loss', color='blue')\n",
    "ax.fill_between(train_steps, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "# Then the eval loss\n",
    "ax.plot(eval_steps, eval_mean, label='Eval Loss', color='orange')\n",
    "ax.fill_between(eval_steps, eval_mean - eval_std, eval_mean + eval_std, alpha=0.2, color='orange')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Averaged Training and Evaluation Loss across 10 seeds')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/plots/loss_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss\n",
    "Getting the test loss is a bit tricky. First we will reconstruct the predictor and remake the dataset with the time points we want to calculate the test loss on. Since setting eval_window=0 means holding out the first 256 time points for validation, this variable is set to 1 so as to avoid rewriting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = reconstruct_predictor()\n",
    "loss_fn = predictor.make_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signals = samples[\"dgp_dataset\"][\"samples\"][819:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2018-01-01 00:00:00')\n",
    "freq = '1h'\n",
    "test_df = data_do_df(test_signals, start_date=start_date, freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ds.Dataset(\n",
    "    data_full=ds.DataCollection(data_loader=lambda: test_df),\n",
    "    start_date=start_date,\n",
    "    train_window=1023,    # almost the full sequence\n",
    "    eval_window=1,        # just 1.\n",
    "    series_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs_fn = functools.partial(\n",
    "    inputs.CreateInputs,\n",
    "    dataset=test_dataset,\n",
    "    batch_size=16,\n",
    "    series_length=256,\n",
    "    weighted_sampling=False,\n",
    "    full_eval=False,\n",
    "    traxify=False,       # raw streams, no need for traxification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stream_fn, eval_stream_fn = test_inputs_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_eval_model = predictor.make_train_eval_model(mode='eval')\n",
    "dummy_batch = next(train_stream_fn(1))\n",
    "raw_eval_model.init(shapes.signature(dummy_batch))\n",
    "eval_model = tl.Accelerate(raw_eval_model)\n",
    "\n",
    "n_batches = 32\n",
    "seeds = [3040 + i for i in range(10)]\n",
    "steps = list(range(500, 5001, 500))\n",
    "\n",
    "# {step: [loss_seed0, loss_seed1, ...]}\n",
    "test_losses_by_step = {step: [] for step in steps}\n",
    "\n",
    "for seed in seeds:\n",
    "    for step in steps:\n",
    "        weights = load_structured_weights(\n",
    "            f\"out/training_result/segmented{seed}_2/model_{step}.weights.npy.gz\"\n",
    "        )\n",
    "        eval_model.weights = weights\n",
    "        \n",
    "        losses = []\n",
    "        stream = train_stream_fn(1)\n",
    "        for i, batch in enumerate(stream):\n",
    "            if i >= n_batches:\n",
    "                break\n",
    "            output = eval_model(batch)\n",
    "            loss = loss_fn(output)\n",
    "            losses.append(float(loss))\n",
    "        \n",
    "        mean_loss = np.mean(losses)\n",
    "        test_losses_by_step[step].append(mean_loss)\n",
    "        print(f\"Seed {seed}, Step {step}: test loss = {mean_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(test_steps, test_mean, label='Test Loss', color='green')\n",
    "ax.fill_between(test_steps, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Averaged Test Loss across 10 seeds')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/plots/test_loss.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = samples[\"dgp_dataset\"][\"samples\"][0]\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(range(896), signal[:896], color='blue', label='Train window')\n",
    "ax.plot(range(896, 1024), signal[896:1024], color='orange', label='Eval window')\n",
    "ax.axvline(x=896, color='black', linestyle='--', linewidth=1.2)\n",
    "ax.set_xlabel('Time point')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Training and evaluation split example')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/plots/signal_windows.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import lr_schedules\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal as scipy_signal\n",
    "def generate_waves(n_signals=256, total_length=1024, wave_type='sine', \n",
    "                   freq_range=(2, 8), noise_std=0.05, seed=67):\n",
    "    np.random.seed(seed)\n",
    "    signals = np.zeros((n_signals, total_length))\n",
    "    t = np.linspace(0, 1, total_length)\n",
    "    for i in range(n_signals):\n",
    "        freq = np.random.uniform(freq_range[0], freq_range[1])\n",
    "        amplitude = np.random.uniform(0.8, 1.2)\n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        if wave_type == 'sine':\n",
    "            wave = amplitude * np.sin(2 * np.pi * freq * t + phase)\n",
    "        elif wave_type == 'triangle':\n",
    "            wave = amplitude * scipy_signal.sawtooth(2 * np.pi * freq * t + phase, width=0.5)\n",
    "        wave += np.random.normal(0, noise_std, total_length)\n",
    "        signals[i] = wave\n",
    "    return signals\n",
    "\n",
    "sine_waves = generate_waves(wave_type='sine')\n",
    "triangle_waves = generate_waves(wave_type='triangle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictors.normalization import PerTsNormalizer\n",
    "normalizer = PerTsNormalizer(regularizer=0.001)\n",
    "\n",
    "for waves, name in [(sine_waves, 'Sine'), (triangle_waves, 'Triangle')]:\n",
    "    norm_data, _, _ = normalizer.normalize(waves)\n",
    "    print(f\"{name} normalized range: [{norm_data.min():.2f}, {norm_data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = int(0.8 * len(sine_waves))\n",
    "n_steps = 5000\n",
    "start_date = pd.Timestamp('2018-01-01 00:00:00')\n",
    "freq = '1h'\n",
    "train_window = 896\n",
    "eval_window = 128\n",
    "series_length = 256\n",
    "batch_size = 16\n",
    "df = data_do_df(sine_waves[:n_signals], start_date=start_date, freq=freq)\n",
    "lr_schedule = functools.partial(\n",
    "    lr_schedules.multifactor,\n",
    "    constant=0.03,\n",
    "    factors='constant * linear_warmup * rsqrt_decay',\n",
    "    warmup_steps=1000\n",
    ")\n",
    "dataset = ds.Dataset(\n",
    "    data_full=ds.DataCollection(data_loader=lambda: df),\n",
    "    start_date=start_date,\n",
    "    train_window=train_window,\n",
    "    eval_window=eval_window,\n",
    "    series_length=series_length,\n",
    ")\n",
    "inputs_iterable = functools.partial(\n",
    "    inputs.CreateInputs,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    weighted_sampling=False,\n",
    "    traxify=True\n",
    ")\n",
    "model_body = functools.partial(\n",
    "    models.TransformerBody,\n",
    "    d_model=128,\n",
    "    d_ff_mul=2,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    max_len=2048,\n",
    "    dropout=0.1,\n",
    "    ff_activation=trax.layers.FastGelu,\n",
    ")\n",
    "predictor_class = functools.partial(\n",
    "    predictors.SerialPredictor,\n",
    "    d_in=128,\n",
    "    vocab_size=10,\n",
    "    precision=3,\n",
    "    significance_decay=0.3,\n",
    "    low=-2.0,  \n",
    "    high=2.0,\n",
    "    normalization=\"per_ts\",\n",
    "    normalization_regularizer=0.001\n",
    ")\n",
    "        \n",
    "loop, predictor = trainer.train(\n",
    "    output_dir=f'./out/training_result/sine67',\n",
    "    inputs=functools.partial(inputs_iterable, full_eval=False),\n",
    "    model_body=model_body,\n",
    "    predictor_class=predictor_class,\n",
    "    optimizer=optimizers.Adam,\n",
    "    lr_schedule=lr_schedule,\n",
    "    n_steps=n_steps,\n",
    "    eval_every=500,\n",
    "    n_eval_batches=32,\n",
    "    calc_eval_loss=True,\n",
    "    seed=67\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = int(0.8 * len(triangle_waves))\n",
    "n_steps = 5000\n",
    "start_date = pd.Timestamp('2018-01-01 00:00:00')\n",
    "freq = '1h'\n",
    "train_window = 896\n",
    "eval_window = 128\n",
    "series_length = 256\n",
    "batch_size = 16\n",
    "df = data_do_df(sine_waves[:n_signals], start_date=start_date, freq=freq)\n",
    "lr_schedule = functools.partial(\n",
    "    lr_schedules.multifactor,\n",
    "    constant=0.03,\n",
    "    factors='constant * linear_warmup * rsqrt_decay',\n",
    "    warmup_steps=1000\n",
    ")\n",
    "dataset = ds.Dataset(\n",
    "    data_full=ds.DataCollection(data_loader=lambda: df),\n",
    "    start_date=start_date,\n",
    "    train_window=train_window,\n",
    "    eval_window=eval_window,\n",
    "    series_length=series_length,\n",
    ")\n",
    "inputs_iterable = functools.partial(\n",
    "    inputs.CreateInputs,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    weighted_sampling=False,\n",
    "    traxify=True\n",
    ")\n",
    "model_body = functools.partial(\n",
    "    models.TransformerBody,\n",
    "    d_model=128,\n",
    "    d_ff_mul=2,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    max_len=2048,\n",
    "    dropout=0.1,\n",
    "    ff_activation=trax.layers.FastGelu,\n",
    ")\n",
    "predictor_class = functools.partial(\n",
    "    predictors.SerialPredictor,\n",
    "    d_in=128,\n",
    "    vocab_size=10,\n",
    "    precision=3,\n",
    "    significance_decay=0.3,\n",
    "    low=-3.0,  \n",
    "    high=3.0,\n",
    "    normalization=\"per_ts\",\n",
    "    normalization_regularizer=0.001\n",
    ")\n",
    "        \n",
    "loop, predictor = trainer.train(\n",
    "    output_dir=f'./out/training_result/triangle68',\n",
    "    inputs=functools.partial(inputs_iterable, full_eval=False),\n",
    "    model_body=model_body,\n",
    "    predictor_class=predictor_class,\n",
    "    optimizer=optimizers.Adam,\n",
    "    lr_schedule=lr_schedule,\n",
    "    n_steps=n_steps,\n",
    "    eval_every=500,\n",
    "    n_eval_batches=32,\n",
    "    calc_eval_loss=True,\n",
    "    seed=68\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_sine = load_structured_weights(f\"out/training_result/sine67/model_5000.weights.npy.gz\")\n",
    "model_body = functools.partial(\n",
    "    models.TransformerBody,\n",
    "    d_model=128,\n",
    "    d_ff_mul=2,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    max_len=2048,\n",
    "    dropout=0.1,\n",
    "    ff_activation=trax.layers.FastGelu,\n",
    ")\n",
    "predictor_sine = predictors.SerialPredictor(\n",
    "    model_body_fn=model_body,\n",
    "    d_in=128,\n",
    "    vocab_size=10,\n",
    "    precision=3,\n",
    "    significance_decay=0.3,\n",
    "    low=-2.0,\n",
    "    high=2.0,\n",
    "    normalization=\"per_ts\",\n",
    "    normalization_regularizer=0.001\n",
    ")\n",
    "n_train = int(0.8 * len(sine_waves))\n",
    "test_sine = sine_waves[n_train:]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "for i in range(4):\n",
    "    context = test_sine[i:i+1, :256]\n",
    "    preds = make_predictions(context, weights_sine, 256, predictor_sine)\n",
    "    ground_truth = test_sine[i, :512]\n",
    "    mse = np.mean((preds[0, 256:] - ground_truth[256:]) ** 2)\n",
    "    axes[i].plot(ground_truth, label='Ground truth', alpha=0.7)\n",
    "    axes[i].plot(preds[0], label='Prediction', linestyle='--')\n",
    "    axes[i].axvline(256, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[i].set_title(f'Sample {i+1}, MSE={mse:.3f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle('Sine predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_triangle = load_structured_weights(f\"out/training_result/triangle68/model_5000.weights.npy.gz\")\n",
    "predictor_triangle = predictors.SerialPredictor(\n",
    "    model_body_fn=model_body,\n",
    "    d_in=128,\n",
    "    vocab_size=10,\n",
    "    precision=3,\n",
    "    significance_decay=0.3,\n",
    "    low=-3.0,\n",
    "    high=3.0,\n",
    "    normalization=\"per_ts\",\n",
    "    normalization_regularizer=0.001\n",
    ")\n",
    "n_train = int(0.8 * len(triangle_waves))\n",
    "test_triangle = triangle_waves[n_train:]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    context = test_triangle[i:i+1, :256]\n",
    "    preds = make_predictions(context, weights_triangle, 256, predictor_triangle)\n",
    "    ground_truth = test_triangle[i, :512]\n",
    "    mse = np.mean((preds[0, 256:] - ground_truth[256:]) ** 2)\n",
    "    axes[i].plot(ground_truth, label='Ground truth', alpha=0.7)\n",
    "    axes[i].plot(preds[0], label='Prediction', linestyle='--')\n",
    "    axes[i].axvline(256, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[i].set_title(f'Sample {i+1}, MSE={mse:.3f}')\n",
    "    axes[i].legend()\n",
    "plt.suptitle('Triangle predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal as scipy_signal\n",
    "def generate_waves(n_signals=256, total_length=1024, wave_type='sine', \n",
    "                   freq_range=(2, 8), noise_std=0.05, seed=67):\n",
    "    np.random.seed(seed)\n",
    "    signals = np.zeros((n_signals, total_length))\n",
    "    t = np.linspace(0, 1, total_length)\n",
    "    for i in range(n_signals):\n",
    "        freq = np.random.uniform(freq_range[0], freq_range[1])\n",
    "        amplitude = np.random.uniform(10, 11)\n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        if wave_type == 'sine':\n",
    "            wave = amplitude * np.sin(2 * np.pi * freq * t + phase)\n",
    "        elif wave_type == 'triangle':\n",
    "            wave = amplitude * scipy_signal.sawtooth(2 * np.pi * freq * t + phase, width=0.5)\n",
    "        wave += np.random.normal(0, noise_std, total_length)\n",
    "        signals[i] = wave\n",
    "    return signals\n",
    "\n",
    "sine_waves = generate_waves(n_signals=256, wave_type='sine')\n",
    "triangle_waves = generate_waves(n_signals=256, wave_type='triangle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check normalized range\n",
    "from predictors.normalization import PerTsNormalizer\n",
    "normalizer = PerTsNormalizer(regularizer=0.001)\n",
    "\n",
    "norm_sine, _, _ = normalizer.normalize(sine_waves)\n",
    "norm_triangle, _, _ = normalizer.normalize(triangle_waves)\n",
    "\n",
    "print(f\"Sine - Raw range: [{sine_waves.min():.2f}, {sine_waves.max():.2f}]\")\n",
    "print(f\"Sine - Normalized range: [{norm_sine.min():.2f}, {norm_sine.max():.2f}]\")\n",
    "print(f\"Triangle - Raw range: [{triangle_waves.min():.2f}, {triangle_waves.max():.2f}]\")\n",
    "print(f\"Triangle - Normalized range: [{norm_triangle.min():.2f}, {norm_triangle.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_body = functools.partial(\n",
    "    models.TransformerBody,\n",
    "    d_model=128,\n",
    "    d_ff_mul=2,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    max_len=2048,\n",
    "    dropout=0.1,\n",
    "    ff_activation=trax.layers.FastGelu,\n",
    ")\n",
    "\n",
    "predictor = predictors.SerialPredictor(\n",
    "    model_body_fn=model_body,\n",
    "    d_in=128,\n",
    "    vocab_size=10,\n",
    "    precision=3,\n",
    "    significance_decay=0.3,\n",
    "    low=-2.0,\n",
    "    high=2.0,\n",
    "    normalization=\"per_ts\",\n",
    "    normalization_regularizer=0.001\n",
    ")\n",
    "weights_sine = load_structured_weights(f\"out/training_result/sine67/model_5000.weights.npy.gz\")\n",
    "n_train = int(0.8 * len(sine_waves))\n",
    "test_sine = sine_waves[n_train:]\n",
    "context_sine = test_sine[:, :256]\n",
    "preds_sine = make_predictions(context_sine, weights_sine, 256, predictor)\n",
    "ground_truth_sine = test_sine[:, :512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    ground_truth = ground_truth_sine[i]\n",
    "    preds = preds_sine[i]\n",
    "    axes[i].plot(ground_truth, label='Ground truth', alpha=0.7)\n",
    "    axes[i].plot(preds, label='Prediction', linestyle='--')\n",
    "    axes[i].axvline(256, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[i].set_title(f'Sample {i+1}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle('Sine predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_body = functools.partial(\n",
    "    models.TransformerBody,\n",
    "    d_model=128,\n",
    "    d_ff_mul=2,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    max_len=2048,\n",
    "    dropout=0.1,\n",
    "    ff_activation=trax.layers.FastGelu,\n",
    ")\n",
    "\n",
    "predictor = predictors.SerialPredictor(\n",
    "    model_body_fn=model_body,\n",
    "    d_in=128,\n",
    "    vocab_size=10,\n",
    "    precision=3,\n",
    "    significance_decay=0.3,\n",
    "    low=-3.0,\n",
    "    high=3.0,\n",
    "    normalization=\"per_ts\",\n",
    "    normalization_regularizer=0.001\n",
    ")\n",
    "weights_triangle = load_structured_weights(f\"out/training_result/triangle68/model_5000.weights.npy.gz\")\n",
    "n_train = int(0.8 * len(triangle_waves))\n",
    "test_triangle = triangle_waves[n_train:]\n",
    "context_triangle = test_triangle[:, :256]\n",
    "preds_triangle = make_predictions(context_triangle, weights_triangle, 256, predictor)\n",
    "ground_truth_triangle = test_triangle[:, :512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    ground_truth = ground_truth_triangle[i]\n",
    "    preds = preds_triangle[i]\n",
    "    axes[i].plot(ground_truth, label='Ground truth', alpha=0.7)\n",
    "    axes[i].plot(preds, label='Prediction', linestyle='--')\n",
    "    axes[i].axvline(256, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[i].set_title(f'Sample {i+1}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle('Triangle predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
